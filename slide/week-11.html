<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>GOVT2901Contemporary Issues in Political Science and International Relations</title>
    <meta charset="utf-8" />
    <meta name="author" content="Francesco Bailo (from slides prepared by Jamie Roberts)" />
    <script src="week-11_files/header-attrs-2.25/header-attrs.js"></script>
    <link href="week-11_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
    </script>
    <style>
    .mjx-mrow a {
      color: black;
      pointer-events: none;
      cursor: default;
    }
    </style>
    <link rel="stylesheet" href="assets/sydney-fonts.css" type="text/css" />
    <link rel="stylesheet" href="assets/sydney.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# GOVT2901</br>Contemporary Issues in Political Science and International Relations
]
.subtitle[
## Week 11</br>Artificial Intelligence
]
.author[
### Francesco Bailo (from slides prepared by Jamie Roberts)
]
.institute[
### The University of Sydney
]
.date[
### Semester 1, 2024 (updated: 2024-05-06)
]

---


background-image: url(https://upload.wikimedia.org/wikipedia/en/6/6a/Logo_of_the_University_of_Sydney.svg)
background-size: 95%



---

## Acknowledgement of Country

I would like to acknowledge the Traditional Owners of Australia and  recognise their continuing connection to land, water and culture. The  University of Sydney is located on the land of the Gadigal people  of the Eora Nation. I pay my respects to their Elders, past and present.

---

## Who am I?
	
Francesco Bailo (francesco.bailo@sydney.edu.au)

I am a Lecturer in the School of Social and Political Sciences, University of Sydney. I am interested in researching forms of political engagement and political talk on social media. I researched the emergence and dynamics of online communities, the role between news organisations and social media, and the interdependence between social media activists and news organisations. I have engaged with and applied quantitative research methods developing expertise in quantitative text analysis (NLP) and network analysis.

This year I am also teaching GOVT6139, SSPS4102 &amp; SSPS6006.

---

# Outline

* What is AI?
* What might AI threaten? 
* The singularity and the alignment / control problem.
* The degradation of humanity.
* AI and traditional security concerns.

---

class: segue-red

# What is AI?


---

- We don't have a shared definition of what artificial intelligence (AI) is or what it should like when we see it.

- Yet, this is not that surprisingly if we think that we don't even have a shared definition of what human intelligence (HI) is.

.center[&lt;img src = "../img/artificial-intelligence.webp" width = "35%"&gt;&lt;/img&gt;]

---

#### The field of "Artificial Intelligence" was created during a workshop at  Dartmouth College in 1956. The proposal for the workshop reads

&gt; "We propose that a 2-month, 10-man study of **artificial intelligence** be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence *can in principle be so precisely described* that a machine can be made to simulate it. An attempt will be made to find **how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves**".

.center[&lt;img src = "../img/artificial-intelligence.webp" width = "15%"&gt;&lt;/img&gt;]



---

But AI researchers got stuck on this:

&gt; every aspect of learning or any other feature of intelligence can in principle be **so precisely described that a machine can be made to simulate it**.

.center[&lt;img src = "../img/symbolic-ai.webp" width = "25%"&gt;&lt;/img&gt;]

... well, after all it is not that simple to "precisely describe" learning or intelligence. It can't be easily break down to discrete steps and rules that a machine could execute.


---

## Symbolic vs Subsymbolic AI

The initial approach to reach AI by flashing out explicit rules is called **symbolic AI**.

&gt; "**Symbolic AI** was originally inspired by mathematical logic as well as by the way people described their conscious thought processes"&lt;sup&gt;1&lt;/sup&gt;

.center[&lt;img src = "../img/symbolic-vs-subsymbolic-ai.webp" width = "20%"&gt;&lt;/img&gt;]

&gt; "In contrast, **subsymbolic** approaches to AI took “inspiration from neuroscience and sought to capture the sometimes-unconscious thought processes"&lt;sup&gt;1&lt;/sup&gt;


.footnote[[1] Mitchell, M. (2019). Artificial intelligence: A guide for thinking humans. Macmillan.]
---

## From Subsymbolic AI to Multilayer neural networks

.center[&lt;img src = "../img/two-layer-network.png" width = "35%"&gt;&lt;/img&gt;]

&gt; "a multilayer neural network can learn to use its hidden units to recognize more abstract features (for example, visual shapes, such as the top and bottom 'circles' on a handwritten 8) than the simple features (for example, pixels) encoded by the input"&lt;sup&gt;1&lt;/sup&gt;

.footnote[[1] Mitchell, M. (2019). Artificial intelligence: A guide for thinking humans. Macmillan.]

---

## Symbolic vs Subsymbolic AI

- Artificial Intelligence applications really took off thanks to the intuition of **Subsymbolic AI** researchers that rule-based intelligent systems are practical only in some situation.

.center[&lt;img src = "../img/symbolic-vs-subsymbolic-ai.webp" width = "25%"&gt;&lt;/img&gt;]

- In most situations, it is definitely more practical to let systems find relevant patterns autonomously (as in neural networks) with limited or no human intervention.  

Of course, this intuition wouldn't be enough without an explosion in data production.

---

## The Data Explosion 

### And the Ascent of Tech Industry Applications for Machine Learning 

.center[&lt;img src = "../img/data-explosion.webp" width = "25%"&gt;&lt;/img&gt;]

* The last "**AI winter**"&lt;sup&gt;1&lt;/sup&gt; ended in early 2000s. 

* The current **AI Spring** was made possible by data availability, mostly produced by the Internet and for the Internet.

.center[.content-box-yellow[Data transformed the value of technologies that had been around for decades.]]

.footnote[[1]Low interest and low funding towards AI applications from industry and governments]

---

## ImageNet and the Wow Moment for Neural Networks

* ImageNet is a database of labelled images networked through categories.

.pull-left[

* Images are collected from the *Internet*;
  
* Images are annotated by human-coders crowd sourced through the *Internet* (using Amazon Mechanical Turk).

* It was presented by Fei-Fei Li and team in **2009**

* In **2010**, ImageNet is used in a challange to assess accuracy of competing algorithms for image classification.  

* In **2012**, *AlexNet* a neural network algorithm outperforms all other competing algorithms, *by far*. 

]

.pull-right[
.center[&lt;img src = "https://upload.wikimedia.org/wikipedia/commons/4/4f/ImageNet_error_rate_history_%28just_systems%29.svg" width = "80%"&gt;&lt;/img&gt;]
]

---

## Neural Networks and the New Data-driven AI Spring  

* AlexNet is a convolutional neural network (CNN). 

* CNN algorithms were "*ancient technology*" in **2012**. 

* CNNs were first applied to image classification in **1989**.

.center[.content-box-green[But in 2012, CNNs would meet data. A lot of data. This was the breakthrough.]] 

Because of this we can't disentangle current AI advances from advances in data collection and processing.

The winning mix that defines the field of competition is 

.content-box-yellow[
$$ Algorithms \times Computation \times Data $$
]

* Computation (i.e., energy and CPUs/GPUs) is the critical interface between algorithms and data.

* Computation and Data are limited resources (algorithms, of course, are not). On computation and data the battle for market and geopolitical dominance is fought and won (or lost).

---

## AI: Narrow and General, Weak and Strong

Recent advances allowed by deep learning techniques - Large Language Models (LLMs) and other generative AIs are all downstream computer innovations made possible by deep learning technologies - have renewed the debate about AI converging on and eventually overcoming HI (i.e., human intellingence).   

* **Narrow** (or **Weak**) AI refers to single-task systems. These systems, such as AlphaGo or ChatGPT, are good (exceptionally and super-humanly good) at one task - but can't really "think" outside of their pre-assigned and pre-designed box.   

* Artificial **General** Intelligence (AGI), is instead a movie-like AI, which can do whatever humans do. AGI should demonstrate "...the ability to explain their results, reason abstractly, and learn in a less data-intensive and more human-like way" (Torres 2019). "Whereas past technologies have been tools used by humans, AGI would constitute an agent in its own right."

* "**Artificial Super Intelligence** (ASI) is a system that could significantly outperform every possible human in all cognitive domains."

---

## But could machine think?

- We don't know! 

#### Would we know if they began thinking?

- We also don't know!

#### So what is AGI supposed to look like? 

- We have no idea.

#### What we seem to agree on is that machines are still not there yet.

.content-box-green[

.center[&lt;large&gt;What we don't agree on is whether machines will ever be there.&lt;/large&gt;]

]

(Of course AI entrepreneurs - who are always active on the *capital* market - instead tend to agree that we will be there sooner than later...)

---

class: segue-red

# Still, what might AI threaten?

---


# Artificial Intelligence and International Security

* Meta question: So, within the realm of International Security, what referent objects could we be concerned with when talking about Artificial Intelligence?
* Some answers (and more questions):
* The **state**
  * Q: What are the threats to the state?
  * Q: Are threats to the state the main concern?
* The **nature of humanity** as we know it?
  * Q: Will AI diminish or augment our humanity?
  * Q: Big philosophical question: is there an ‘essential’ humanity that ought to be secured? (Might augmenting our humanity be a threat to our humanity?)
* The **existence of all humanity** (Bostrom's existential threat scenario)
  * Q: How might AI threaten the existence of all humanity?



---
class: segue-red

# The Doomsday Clock


---


# The Bulletin of the Atomic Scientists

Question: Do you know  _The Bulletin of the Atomic Scientists?_

&gt; "The Bulletin’s website, iconic Doomsday Clock, and regular events help advance actionable ideas at a time when  __technology is outpacing our ability to control it__".

Four key areas: 

* nuclear risk, 
* climate change,
* biological threats (accidental and deliberate misuse of biotech - AI could of course accelerate discoveries);
* disruptive technologies (such as AI).  

&gt; "What connects these topics is a driving belief that because humans created them, __we can control them__ ."

---

# A few question

.pull-left[
* Question: Is technology *outpacing* our ability to control it?

* Question: What do you think about the three security threats listed? Are they the ‘big three’ (nuclear, climate, bio, disruptive tech) at the moment?

* Question: Which is the greatest threat?
]

.pull-right[
.center[&lt;img src = 'https://pictures.abebooks.com/isbn/9780066620817-us.jpg' width = "60%"&gt;&lt;/img&gt;]
]

---
class: segue-red

# Part 1: The Singularity


---

class: segue

# A Brief history of the concept of 'The Singularity'


---


# Ulam and Von Neumann

1957: **Stanislaw Ulam** on **John Von Neumann**&lt;sup&gt;1&lt;/sup&gt;

&gt; "One conversation [with Von Neumann] centered on the ever accelerating progress of technology and changes in the mode of human life, which gives the appearance of approaching some essential *singularity* in the history of the race beyond which human affairs, as we know them, could not continue."

Question: What does it mean: "human affairs, as we know them, could not continue"? Ought we to secure human affairs as we know them? Would a radical change be a security threat?

.footnote[They were both mathematicians, nuclear physicists and computer scientists. They were also both part of the Manhattan Project to develop the first nuclear weapon during WWII.]

---


# Vernor Vinge&lt;sup&gt;1&lt;/sup&gt; predicts the singularity in 2023 

in his *The coming technological singularity* published in 1993.

&gt; Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended."

.footnote[[1] Mathematician, computer scientist and science fiction writer.]

---

# Vernor Vinge: a beautiful description of our approach to the singularity

&gt; "When greater-than-human intelligence drives progress, that  __progress will be much more rapid__ . In fact, there seems no reason why progress itself would not involve the creation of still more intelligent entities -- on a still-shorter time scale. The best analogy that I see is with the evolutionary past: Animals can adapt to problems and make inventions, but often  __no faster than natural selection can do its work__  --  __the world acts as its own simulator in the case of natural selection__ . 

&gt; We humans have the ability to  __internalize the world and conduct "what if's" in our heads__ ; we can solve many problems thousands of times faster than natural selection. 

&gt; Now, by creating the means to execute those simulations at much higher speeds, we are entering a regime as radically different from our human past as we humans are from the lower animals."

---

## The stages of adaptation

* Natural selection (very slow – millions of years)

* Technological innovation driven by human imagination (increasingly fast – from thousands of years to single years and less)

* Using AI to execute simulations (once it begins, intelligence will grow *exponentially*)

Key to understand the singularity event, is the idea of **exponential growth**, which is difficult to grasp as all non-linear processes. Think about the weekly spread of the COVID-19 infection across a population or the chessboard problem.&lt;sup&gt;1&lt;/sup&gt;  

.footnote[[1] "On the 64th square of the chessboard alone, there would be 2^63 = 9,223,372,036,854,775,808 grains, more than two billion times as many as on the first half of the chessboard."]

.pull-left[.center[&lt;img src = 'https://upload.wikimedia.org/wikipedia/commons/c/cd/Wheat_and_chessboard_problem.jpg' width = "50%"&gt;&lt;/img&gt;]]

.pull-right[.center[&lt;img src = 'https://upload.wikimedia.org/wikipedia/commons/e/e7/Wheat_Chessboard_with_line.svg' width = "50%"&gt;&lt;/img&gt;]]

---

# Vernor Vinge: framing the singularity in terms of security

&gt; "From the human point of view this change will be a  __throwing away of all the previous rules__ , perhaps in the blink of an eye, an exponential runaway beyond any hope of control. Developments that before were thought might only happen in 'a million years' (if ever) will likely happen in the next century..." 

&gt; "I think it's fair to call this event a singularity ('the Singularity' for the purposes of this paper). It is a point where our models must be discarded and a new reality rules."

&gt; "__As we move closer and closer to this point, it will loom vaster and vaster over human affairs till the notion becomes a commonplace__ . Yet when it finally happens it may still be a great surprise and a greater unknown."

.content-box-yellow[As with Von Neumann, some sort of dramatic but vague events.]

---

## A few questions

Question: Is it a security concern that all of the previous rules will be thrown away? Does security come from having rules / models / schemas?

Question: Does security depend upon slow changes?

Question: Does security depend upon human’s controlling change?


---


# Machine Intelligence Research Institute

2005: Eliezer Yudkowsky&lt;sup&gt;1&lt;/sup&gt; founds the Machine Intelligence Research Institute (Yudkowsky has become a singularity Doomer)

&gt; "The AIoes not hate you, nor does it love you, but you are made out of atoms which it can use for something else."

.footnote[[1] Artificial intelligence researcher]
---


# Kurzweil: The anti Doomer: no security threat

2005: Ray Kurzweil:&lt;sup&gt;1&lt;/sup&gt;  _The Singularity Is Near: When Humans Transcend Biology_

Kurzweil: evolution moves towards "greater complexity, greater elegance, greater knowledge, greater intelligence, greater beauty, greater creativity, and greater levels of subtle attributes such as love".

Question: What are your thoughts about this teleological account? Note that he paints the singularity as not being a security threat – in the deepest sense of ‘security’.

Question: Ought we to look forward to the transformations brought about by the singularity?

.footnote[[1] Computer scientist.]

---

# Kurzweil's wild singularity

&gt; "a future period during which the pace of technological change will be so rapid, its impact so deep, that human life will be irreversibly transformed"

&gt; "a unique event with ... singular implications"

&gt; "an event capable of rupturing the fabric of human history."

&gt; By the 2020s molecular assembly will provide tools to effectively combat poverty, clean up our environment, overcome disease, [and] extend human longevity.&lt;sup&gt;1&lt;/sup&gt;

.footnote[[1]Kurzweil, Ray (2005). The Singularity is Near. New York: Viking Books.]

---

# Kurzweil's wild singularity (continued)

&gt; By the end of the 2030s … brain implants based on massively distributed intelligent nanobots will greatly expand our memories and otherwise vastly improve all our sensory, pattern-recognition, and cognitive abilities.

&gt; Uploading a human brain means scanning all of its salient details and then reinstantiating those details into a suitably powerful computational substrate. The end of the 2030s is a conservative projection for successful [brain] uploading.

&gt; A computer will pass the Turing test by 2029.

&gt; As we get to the 2030s, artificial consciousness will be very realistic. That’s what it means to pass the Turing test.

&gt; I set the date for the Singularity … as 2045. The nonbiological intelligence created in that year will be one billion times more powerful than all human intelligence today.

.footnote[[1]Kurzweil, Ray (2005). The Singularity is Near. New York: Viking Books.]


---


# Bostrom’s Superintelligence – not upbeat

2014 (Elon Musk gave Bostrom’s institute  £1m, which shut down in 2024)

Geist (2015) on Bostrom:

‘Intelligent machines are “quite possibly the most important and most daunting challenge humanity has ever faced.”’

Question: What do you think about this claim?

And Geist again: ‘With intellectual powers beyond human comprehension, he [Bostrom] prognosticates, self-improving artificial intelligences could effortlessly enslave or destroy Homo sapiens if they so wished.’

Question: Alarmism?

---

https://thebulletin.org/2015/08/is-artificial-intelligence-really-an-existential-threat-to-humanity/#post-heading


---


# More of Geist’s skepticism in response to Bostrom

Geist: ‘The risks of self-improving intelligent machines are grossly exaggerated and ought not serve as a distraction from the existential risks we already face…’

Question: Do you agree? Why might Geist be so dismissive?


---


# Geist’s incredulity (remember: 2015)

Geist: ‘Simply depriving AIs of information about the world is not adequate to render them safe, he [Bostrom] claims, as they might be able to accomplish such feats as solving extremely complex problems in physical science  __without the need to carry out real-world experiments__ . In a scenario borrowed from Yudkowsky,  __Bostrom posits that a superintelligence might “crack the protein folding problem”__  and then manipulate a gullible human into mixing mail-ordered synthesized proteins “in a specified environment” to create “a very primitive ‘wet’ nanosystem, which, ribosome-like, is capable of accepting external instructions; perhaps patterned acoustic vibrations delivered by a speaker attached to the beaker.” It could then employ this system to bootstrap increasingly sophisticated nanotechnologies, and “at a pre-set time, nanofactories producing nerve gas or target-seeking mosquito-like robots might then burgeon forth simultaneously from every square meter of the globe (although more effective ways of killing could probably be devised by a machine with the technology research superpower).” This scenario doesn't just strain a reader’s credulity; it also implies a fanciful understanding of the nature of technological development in which “genius” can somehow substitute for hard work and countless intermediate failures. In the real world, the “lone genius inventor” is a myth; even smarter-than-human  __AIs could never escape the tedium of an iterative research and development process__ .’

Big question: Could AI’s escape the tedium of an iterative research and development process?

Aaaaaaaand…


---


# From New Scientist 28 July 2022

2022 ‘DeepMind's protein-folding AI cracks biology's biggest problem. Artificial intelligence firm DeepMind has transformed biology by predicting the structure of nearly all proteins known to science in just 18 months, a breakthrough that will speed drug development and revolutionise basic science.’

---

https://www.newscientist.com/article/2330866-deepminds-protein-folding-ai-cracks-biologys-biggest-problem/


---


# ChatGPT is launched… (narrow) AI crosses a threshold

2022 November 30: ChatGPT is launched

Me: ‘Hey GPT4, to what extent ought we to be concerned about the threat posed by AI?’

GPT4:

Superintelligent AI: This is the idea, popularized by figures like Nick Bostrom and Elon Musk, that an AI with goals misaligned with human values could be highly detrimental.

Bias and Discrimination: Machine learning models can inherit and amplify the biases present in their training data.

Economic Impacts: Automation, driven by AI, can result in job losses in various sectors, potentially leading to economic disparity.

Weaponization: AI-powered autonomous weapons could change the nature of warfare, making conflicts more impersonal and possibly more frequent.

Loss of Privacy: AI-driven surveillance systems could lead to unprecedented invasions of privacy. The rise of deepfakes, AI-generated fake videos, is another concern.

Dependence: Over-reliance on AI systems might diminish human skills and our ability to make decisions without technological assistance.

Opaque Decision-making: Many advanced AI models are "black boxes," making it difficult to understand and explain their decisions. This opacity can be problematic in sectors where accountability is crucial.


---


# Early 2023: ‘Pause Giant AI Experiments: An Open Letter’

2023 (March 22) Future of Life Institute

Letter signed by many significant figures in AI:

‘AI systems with human-competitive intelligence can pose profound risks to society and humanity, as shown by extensive research and acknowledged by top AI labs.[2] As stated in the widely-endorsed [Asilomar AI Principles](https://futureoflife.org/open-letter/ai-principles/),  _Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources_ .’

---

https://futureoflife.org/open-letter/pause-giant-ai-experiments/


---


# And this…

Contemporary AI systems are now becoming human-competitive at general tasks, and we must ask ourselves: [1]  _Should_  we let machines flood our information channels with propaganda and untruth? [2]  _Should_  we automate away all the jobs, including the fulfilling ones? [3]  _Should_  we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? [4]  _Should_  we risk loss of control of our civilization? [5] Such decisions must not be delegated to unelected tech leaders.  __Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable.__

Question: Consider the above claims (1-5).

Question: What if anything should be done about AI regulation?


---


# The call for establishing shared safety protocols + hitting ‘pause’

‘AI labs and independent experts should use this pause to jointly develop and implement a set of shared safety protocols for advanced AI design and development that are rigorously audited and overseen by independent outside experts.’

And this important claim: ‘Society has hit pause on other technologies with potentially catastrophic effects on society.’

Question: What technologies has society hit ‘pause’ on? What technologies has society not hit ‘pause’ on? What accounts for the differences?


---


# Since March 2023

Question: What do you think has happened with AI research since?

Answer:

From  _Wired _ (September 28 2023) _:_

‘As you may have noticed, the letter did not result in a pause in AI development, or even a slow down to a more measured pace. Companies have instead accelerated their efforts to build more advanced AI.

Elon Musk, one of the most prominent signatories, didn’t wait long to ignore his own call for a slowdown. In July he announced xAI, a new company he said would seek to go beyond existing AI and compete with OpenAI, Google, and Microsoft. And many Google employees who also signed the open letter have stuck with their company as it prepares to release an AI model called Gemini, which boasts broader capabilities than OpenAI’s GPT-4.’

Question: Why can’t we stop? What ought to be done?

---

https://www.wired.com/story/fast-forward-elon-musk-letter-pause-ai-development/


---


Part 1.5: The alignment and control problems


---


# Goals and alignment

Question: What are our (humanity’s) goals?

Question: What will be that goals of AI? Where will these goals come from?

Question: How will we be able to ensure that the goals of AI will ‘align’ with human goals? (Do we have a similar problem already even with other humans?)

Question: How might a lack of alignment – even a small lack of alignment – cause big problems?


---


# The control problem in more detail, drawing on Torres 2019: ‘The possibility and risks of artificial general intelligence’

The ‘orthogonality thesis’

This is a component of the ‘paperclip problem’: AGI will continue with some ‘silly’ goal.

The instrumental convergence thesis

This is another component of the ‘paperclip problem’: AGI will do whatever it can to pursue its original goal (including wiping out humans).

The complexity of value thesis

This refers to the challenge of programming in human values. Human values are complex.


---


The perplexity of value thesis

The issue here is that we don’t exactly know what human values are. General question: Again: What will motivate AGI (how will AGI derive its goals)?

The fragility of value thesis

Question: Might it be the case that one small error in programming human values causes serious divergence?


---


Part 2: Degradation of human experience


---


# Remember Ray Kurzweil

evolution moves towards ‘greater complexity, greater elegance, greater knowledge, greater intelligence, greater beauty, greater creativity, and greater levels of subtle attributes such as love’.


---


# A different kind of threat

Question: Again: What is the best way for humans to live?

Question: Has our humanity been every been diminished by, say, technology or the nature of the societies we create?

Question: Can AI diminish our humanity?

Question: Is the point here that what must be secured is our freedom? (Interesting that freedom and security are sometimes seen to be in tension).


---


# The real threat: degradation of human experience

Nowak et al.: ‘The real danger of AI lies not in sudden apocalypse, but in  __the gradual degradation and disappearance of what make human experience and existence meaningful__ .’

And later: ‘Instead, humans may gradually become  __passive__  elements of an emerging global socio-technical system — a system composed of machines, algorithms, sensors and actuators, AI programs, and humans interacting in the globally present Internet, an Internet that is ever-present due to mobile technologies and ambient intelligence.’

And later: ‘The essence of this question is:  __what are the real chances for humans to break-out of the choices dictated by the system of which they are an element__ ? What are their chances to retain the capacity for independent, critical thinking? Can they retain emotions and feelings dictated by their internal processes, rather than those dictated by information tailored to manipulate their emotions, or by autonomous decision-making?’

And later: ‘Another dimension of this question is “ __to what degree do interactions and contacts between individuals retain a human character, characterized by the intrinsic value of the inner experiences of other humans__ ?”

Question: Is it possible that we are subtly losing our humanity to opaque global forces? Does this humanity need to be  _secured_ ?

Question: More broadly, do you think that life is becoming more or less meaningful, or staying about the same? How might we measure such things?

Question: Does some of this also just sound like Marxism?


---


# Degradation of cognitive capacities

‘The rapid development of AI that  __replaces rather than augments __ human intelligence can also dramatically diminish the capacity for cognition of the human race. In this scenario, human information processing is delegated to AI, and humans just get answers. They don’t gain understanding of the knowledge and processing rules that led to the solutions.’

Question: Is this degradation of human capabilities another thing that needs to be  __secured__  against?


---


Part 3: AI Arms Race


---


# Traditional security concerns: The ‘missile gap’ in AI

Henshall: ‘The reasons that a pause in AI won’t happen are multifold—and are about more than just the research itself. Critics of the proposed pause argue that regulating or restricting AI  __would help China pull ahead in AI development__ , causing the United States to lose its military and economic edge.’

Question: Is AI destined to be (if it isn’t already) just another arms race?


---


# AI and hegemony: the ‘singleton’

Geist referring to Bostrom: ‘According to Superintelligence, the lucky beneficiary of an intelligence explosion would probably gain a “decisive strategic advantage” which it could then employ to form a “singleton,” defined as “a world order in which there is at the global level a single decision-making agency.”’

Question: Might AI merely continue the traditional pursuit of hegemony (empire)? Or might it fundamentally change the world order?

Question: Will an AI intelligence explosion align with traditional state boundaries? i.e. Will or even can it be contained by the state?


---


# The real threat of AI: amplifying human stupidity?

Geist: ‘But if artificial intelligence might not be tantamount to “summoning the demon” (as Elon Musk colourfully described it), AI-enhanced technologies might still be extremely dangerous due to their potential for  __amplifying human stupidity__ . The AIs of the foreseeable future need not think or create to sow mass unemployment, or enable new weapons technologies that undermine precarious strategic balances. Nor does artificial intelligence need to be smarter than humans to threaten our survival—all it needs to do is make the technologies behind familiar 20th-century existential threats faster, cheaper, and more deadly.’

Question: Is this the real threat? i.e. AI will amplify human stupidity?



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="assets/remark-zoom.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
